{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd039e-3207-4b4d-97c5-7981c59d6011",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "\n",
    "# Libraries\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import MllamaForConditionalGeneration, AutoProcessor\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950ee9f7-51b6-48e8-bc98-bf80d4a6049f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from hugging face\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "\n",
    "model = MllamaForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75421e52-e819-4f74-b370-a4885478fb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set target dataset and file locations\n",
    "target_caption_dataset_filename = \"./data/caption-dataset/annotations/train.json\"\n",
    "image_folder = \"./data/caption-dataset/train/\"\n",
    "\n",
    "# get image quality annotations\n",
    "target_image_quality_dataset_filename = (\n",
    "    \"./data/image-quality-assessment/annotations/train.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544e5652-62fa-4d42-b938-e494f69f3f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get images and annotations in one dataframe\n",
    "image_annotation_df = None\n",
    "with open(target_caption_dataset_filename) as f:\n",
    "    # load caption dataset\n",
    "    caption_dataset_json = json.load(f)\n",
    "\n",
    "    # combine image files and annotations\n",
    "    images_df = pd.DataFrame.from_dict(caption_dataset_json[\"images\"])\n",
    "    annotations_df = pd.DataFrame.from_dict(caption_dataset_json[\"annotations\"])\n",
    "    grouped_annotations = (\n",
    "        annotations_df.groupby([\"image_id\"]).agg(tuple).map(list).reset_index()\n",
    "    )\n",
    "    image_annotation_df = images_df.merge(\n",
    "        grouped_annotations[[\"image_id\", \"caption\", \"is_precanned\", \"is_rejected\"]],\n",
    "        left_on=\"id\",\n",
    "        right_on=\"image_id\",\n",
    "    )\n",
    "\n",
    "    # vizwiz_url is broken, so fix with https://vizwiz.cs.colorado.edu/*\n",
    "    image_annotation_df[\"vizwiz_url\"] = image_annotation_df[\"vizwiz_url\"].apply(\n",
    "        lambda x: x.replace(\n",
    "            \"https://ivc.ischool.utexas.edu/\", \"https://vizwiz.cs.colorado.edu/\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "image_annotation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a662df-fa29-4f57-94c3-005d652f0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image quality\n",
    "image_quality_annotation_df = None\n",
    "with open(target_image_quality_dataset_filename) as f:\n",
    "    # load image quality annotation dataset\n",
    "    image_quality_dataset_json = json.load(f)\n",
    "    image_quality_df = pd.DataFrame.from_dict(image_quality_dataset_json)\n",
    "\n",
    "    # expand object of flaws into individual columns and rename\n",
    "    image_quality_df = pd.concat(\n",
    "        [\n",
    "            image_quality_df.drop([\"flaws\"], axis=1),\n",
    "            pd.json_normalize(image_quality_df[\"flaws\"]),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    image_quality_df.rename(\n",
    "        columns={\n",
    "            \"FRM\": \"framing\",\n",
    "            \"BLR\": \"blur\",\n",
    "            \"DRK\": \"too dark\",\n",
    "            \"BRT\": \"too bright\",\n",
    "            \"OBS\": \"obstruction\",\n",
    "            \"OTH\": \"other\",\n",
    "            \"NON\": \"no issue\",\n",
    "            \"ROT\": \"rotation\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "image_quality_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6505869-fbbf-4235-99a4-526808e03416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine image and quality datasets together\n",
    "image_captioning_input = image_annotation_df.merge(\n",
    "    image_quality_df, left_on=\"file_name\", right_on=\"image\"\n",
    ").drop([\"image\"], axis=1)\n",
    "\n",
    "image_captioning_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b93ff69-47f6-4cc8-8051-799405157e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter input for only blurred images\n",
    "filtered_images_df = image_captioning_input[image_captioning_input[\"blur\"] >= 3]\n",
    "filtered_images_df[\"model_caption\"] = \"\"\n",
    "\n",
    "dataset_to_caption = filtered_images_df.to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefae5d-e838-45b1-a60c-06a653dc751d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model prompt\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a program designed to help blind and low-vision users understand images. When asked about the image, generate accessible image description that includes key visual and contextual details of the image for blind and low-vision people. Focus on the following principles: Clarity and Conciseness: Use simple, straightforward language to describe the main subjects and their relationships.; Relevance: Highlight only essential visual elements that contribute to understanding the image or its purpose.; Context: Provide contextual information when necessary, such as emotional tone, setting, or action. Avoid assumptions or subjective interpretations.; Specificity: Include important details like colors, shapes, textures, or text visible in the image, if relevant. Avoid overly general terms or unnecessary details.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Can you please tell me what is in this image?\",\n",
    "                # Use simple, straightforward language to describe the main subjects and their relationships. Highlight only essential visual elements. Provide contextual information when necessary. Avoid assumptions, subjective interpretations, or generalities.\n",
    "            },\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "input_text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "for index, row in enumerate(tqdm(dataset_to_caption)):\n",
    "    # get image for current annotation\n",
    "    image_file = os.path.join(image_folder, dataset_to_caption[index][\"file_name\"])\n",
    "    image = Image.open(image_file)\n",
    "\n",
    "    # setup model inputs\n",
    "    inputs = processor(\n",
    "        image, input_text, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    # generate output and store in dict\n",
    "    output = model.generate(**inputs, max_new_tokens=50)\n",
    "    decoded_output = processor.decode(output[0])\n",
    "    clean_output = decoded_output.split(\"<|end_header_id|>\")[-1].strip()\n",
    "    dataset_to_caption[index][\"model_caption\"]\n",
    "\n",
    "    # write file if 50 rows have been processed\n",
    "    if (index + 1) % 50 == 0 or index == len(dataset_to_caption) - 1:\n",
    "        with open(\n",
    "            \"./data/labeled-data/labeled-data_{}.csv\".format(index + 1),\n",
    "            \"w\",\n",
    "            encoding=\"utf8\",\n",
    "            newline=\"\",\n",
    "        ) as output_file:\n",
    "            fc = csv.DictWriter(\n",
    "                output_file,\n",
    "                fieldnames=dataset_to_caption[0].keys(),\n",
    "            )\n",
    "            fc.writeheader()\n",
    "            fc.writerows(dataset_to_caption)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
