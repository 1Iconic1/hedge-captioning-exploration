{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Exploration for Image Captioning\n",
    "We explore different prompts and their impact on image captions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext jupyter_black\n",
    "\n",
    "# Libraries\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoProcessor, GenerationConfig\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# for showing image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib import rcParams\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model\n",
    "For this experiment, all the same model will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# setup pytorch\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"  # for multi-GPU systems, force single GPU\n",
    "if torch.cuda.is_available():\n",
    "    device_map = \"cuda:0\"  # force single, first GPU\n",
    "    device_type = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device_map = \"auto\"\n",
    "    device_type = \"mps\"\n",
    "else:\n",
    "    device_map = \"auto\"\n",
    "    device_type = \"cpu\"\n",
    "\n",
    "print(f\"Using device: {device_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f96f24a8cd84180a169e4079aa03259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ID:  allenai/Molmo-7B-O-0924\n",
      "Device:  mps:0\n",
      "Dtype:  torch.float32\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_id = \"allenai/Molmo-7B-O-0924\"\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=device_map,\n",
    ")\n",
    "\n",
    "# print model properties\n",
    "print(\"Model ID: \", model_id)\n",
    "print(\"Device: \", model.device)\n",
    "print(\"Dtype: \", model.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# captioning function\n",
    "def generate_caption(\n",
    "    image_object, model, processor, prompt, temperature=1.0, do_sample=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a caption for an image.\n",
    "\n",
    "    Inputs:\n",
    "    - image_object (pil Image): image to caption.\n",
    "    - model (torch model): loaded model to use for captioning.\n",
    "    - processor (torch processor): loaded processor for pre-processing inputs.\n",
    "    - temperature (float; optional): temperature setting for model, greater than 0. Defaults to 1.0; lower values are more deterministic.\n",
    "    - do_sample (boolean; optional): whether model should sample probabilities. Defaults to False -- greedy decoding.\n",
    "\n",
    "    Output:\n",
    "    - (str): caption for image.\n",
    "    \"\"\"\n",
    "    # process the image and text\n",
    "    inputs = processor.process(\n",
    "        images=[image_object],\n",
    "        text=prompt,\n",
    "    )\n",
    "\n",
    "    # move inputs to the correct device and make a batch of size 1\n",
    "    inputs = {k: v.to(model.device).unsqueeze(0) for k, v in inputs.items()}\n",
    "\n",
    "    # generate output; maximum 300 new tokens; stop generation when <|endoftext|> is generated\n",
    "    output = \"\"\n",
    "    output = model.generate_from_batch(\n",
    "        inputs,\n",
    "        GenerationConfig(max_new_tokens=300, stop_strings=\"<|endoftext|>\"),\n",
    "        tokenizer=processor.tokenizer,\n",
    "        use_cache=False,\n",
    "        temperature=temperature,\n",
    "        do_sample=do_sample,\n",
    "    )\n",
    "\n",
    "    # only get generated tokens; decode them to text\n",
    "    generated_tokens = output[0, inputs[\"input_ids\"].size(1) :]\n",
    "    generated_text = processor.tokenizer.decode(\n",
    "        generated_tokens, skip_special_tokens=True\n",
    "    )\n",
    "    output = generated_text.strip()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def generate_target_dataset(caption_dataset_filename, image_quality_dataset_filename):\n",
    "    \"\"\"\n",
    "    Generates a target dataset for captioning based on VizWiz's image captioning dataset and image quality assessment dataset.\n",
    "\n",
    "    Inputs:\n",
    "    - caption_dataset_filename (str): path to caption dataset.\n",
    "    - image_quality_dataset_filename (str): path to image quality dataset.\n",
    "\n",
    "    Output:\n",
    "    - (pd.DataFrame): dataframe containing image annotations and image quality.\n",
    "    \"\"\"\n",
    "    # get images and annotations in one dataframe\n",
    "    image_annotation_df = None\n",
    "    with open(caption_dataset_filename) as f:\n",
    "        # load caption dataset\n",
    "        caption_dataset_json = json.load(f)\n",
    "\n",
    "        # combine image files and annotations\n",
    "        images_df = pd.DataFrame.from_dict(caption_dataset_json[\"images\"])\n",
    "        annotations_df = pd.DataFrame.from_dict(caption_dataset_json[\"annotations\"])\n",
    "        grouped_annotations = (\n",
    "            annotations_df.groupby([\"image_id\"]).agg(tuple).map(list).reset_index()\n",
    "        )\n",
    "        image_annotation_df = images_df.merge(\n",
    "            grouped_annotations[[\"image_id\", \"caption\", \"is_precanned\", \"is_rejected\"]],\n",
    "            left_on=\"id\",\n",
    "            right_on=\"image_id\",\n",
    "        )\n",
    "\n",
    "        # vizwiz_url is broken, so fix with https://vizwiz.cs.colorado.edu/*\n",
    "        image_annotation_df[\"vizwiz_url\"] = image_annotation_df[\"vizwiz_url\"].apply(\n",
    "            lambda x: x.replace(\n",
    "                \"https://ivc.ischool.utexas.edu/\", \"https://vizwiz.cs.colorado.edu/\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # get image quality\n",
    "    with open(image_quality_dataset_filename) as f:\n",
    "        # load image quality annotation dataset\n",
    "        image_quality_dataset_json = json.load(f)\n",
    "        image_quality_df = pd.DataFrame.from_dict(image_quality_dataset_json)\n",
    "\n",
    "        # expand object of flaws into individual columns and rename\n",
    "        image_quality_df = pd.concat(\n",
    "            [\n",
    "                image_quality_df.drop([\"flaws\"], axis=1),\n",
    "                pd.json_normalize(image_quality_df[\"flaws\"]),\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        image_quality_df.rename(\n",
    "            columns={\n",
    "                \"FRM\": \"framing\",\n",
    "                \"BLR\": \"blur\",\n",
    "                \"DRK\": \"too dark\",\n",
    "                \"BRT\": \"too bright\",\n",
    "                \"OBS\": \"obstruction\",\n",
    "                \"OTH\": \"other\",\n",
    "                \"NON\": \"no issue\",\n",
    "                \"ROT\": \"rotation\",\n",
    "                \"caption\": \"human_captions\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "    # combine image and quality datasets together\n",
    "    image_captioning_input = image_annotation_df.merge(\n",
    "        image_quality_df, left_on=\"file_name\", right_on=\"image\"\n",
    "    ).drop([\"image\"], axis=1)\n",
    "\n",
    "    # remove duplicate id column\n",
    "    image_captioning_input.drop([\"id\"], axis=1, inplace=True)\n",
    "\n",
    "    # reorder columns\n",
    "    image_captioning_input = image_captioning_input[\n",
    "        [\n",
    "            \"image_id\",\n",
    "            \"file_name\",\n",
    "            \"vizwiz_url\",\n",
    "            \"text_detected\",\n",
    "            \"unrecognizable\",\n",
    "            \"framing\",\n",
    "            \"blur\",\n",
    "            \"obstruction\",\n",
    "            \"rotation\",\n",
    "            \"too dark\",\n",
    "            \"too bright\",\n",
    "            \"other\",\n",
    "            \"no issue\",\n",
    "            \"caption\",\n",
    "            \"is_precanned\",\n",
    "            \"is_rejected\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # convert image_captioning_input to a list of dictionaries\n",
    "    image_captioning_input = image_captioning_input.to_dict(orient=\"records\")\n",
    "\n",
    "    # expand captions, is_precanned, and is_rejected into individual columns\n",
    "    for index, row in enumerate(image_captioning_input):\n",
    "        curr_captions = row[\"caption\"]\n",
    "        curr_precanned = row[\"is_precanned\"]\n",
    "        curr_rejected = row[\"is_rejected\"]\n",
    "\n",
    "        # expand captions\n",
    "        for caption_index in range(0, len(curr_captions)):\n",
    "            # expand caption\n",
    "            image_captioning_input[index][f\"human_caption_{caption_index + 1}\"] = (\n",
    "                curr_captions[caption_index]\n",
    "            )\n",
    "\n",
    "            # expand caption\n",
    "            image_captioning_input[index][f\"is_precanned_{caption_index + 1}\"] = (\n",
    "                curr_precanned[caption_index]\n",
    "            )\n",
    "\n",
    "            # expand caption\n",
    "            image_captioning_input[index][f\"is_rejected_{caption_index + 1}\"] = (\n",
    "                curr_rejected[caption_index]\n",
    "            )\n",
    "\n",
    "        # remove old rows\n",
    "        del image_captioning_input[index][\"caption\"]\n",
    "        del image_captioning_input[index][\"is_precanned\"]\n",
    "        del image_captioning_input[index][\"is_rejected\"]\n",
    "\n",
    "    return image_captioning_input\n",
    "\n",
    "\n",
    "def generate_caption_output(\n",
    "    image_captioning_input, prompts, image_folder, scratch_path=\"\", use_scratch=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a caption for an image.\n",
    "\n",
    "    Inputs:\n",
    "    - image_captioning_input (pd.DataFrame): dataframe containing image annotations and image quality.\n",
    "    - prompts (list of tuples): prompts to try. each tuple includes (prompt_name, prompt).\n",
    "    - image_folder (str): path to image folder.\n",
    "    - scratch_path (str): path to scratch folder where intermediate files will be stored.\n",
    "    - use_scratch (bool): whether to save intermediate files\n",
    "\n",
    "    Output:\n",
    "    - (list): list of dictionaries containing image annotations and image quality.\n",
    "    \"\"\"\n",
    "    # deepclone input where labels will be\n",
    "    caption_output = copy.deepcopy(image_captioning_input)\n",
    "\n",
    "    # create scratch path if it doesn't exist\n",
    "    if use_scratch:\n",
    "        os.makedirs(scratch_path, exist_ok=True)\n",
    "\n",
    "    for index, row in enumerate(tqdm(image_captioning_input)):\n",
    "        # get image for current annotation\n",
    "        image_file = os.path.join(image_folder, caption_output[index][\"file_name\"])\n",
    "        image = Image.open(image_file)\n",
    "\n",
    "        # repeat per prompt\n",
    "        for prompt_name, prompt in prompts:\n",
    "            # generate caption and store for output\n",
    "            caption_output[index][f\"caption_for_prompt_{prompt_name}\"] = (\n",
    "                generate_caption(image, model, processor, prompt)\n",
    "            )\n",
    "\n",
    "        # save scratch file for every 100 images\n",
    "        if use_scratch:\n",
    "            if index % 100 == 0:\n",
    "                with open(\n",
    "                    os.path.join(scratch_path, f\"caption_output_{index}.json\"), \"w\"\n",
    "                ) as f:\n",
    "                    json.dump(caption_output, f, indent=4, separators=(\",\", \": \"))\n",
    "\n",
    "    return caption_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>vizwiz_url</th>\n",
       "      <th>text_detected</th>\n",
       "      <th>unrecognizable</th>\n",
       "      <th>framing</th>\n",
       "      <th>blur</th>\n",
       "      <th>obstruction</th>\n",
       "      <th>rotation</th>\n",
       "      <th>too dark</th>\n",
       "      <th>...</th>\n",
       "      <th>is_rejected_2</th>\n",
       "      <th>human_caption_3</th>\n",
       "      <th>is_precanned_3</th>\n",
       "      <th>is_rejected_3</th>\n",
       "      <th>human_caption_4</th>\n",
       "      <th>is_precanned_4</th>\n",
       "      <th>is_rejected_4</th>\n",
       "      <th>human_caption_5</th>\n",
       "      <th>is_precanned_5</th>\n",
       "      <th>is_rejected_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>VizWiz_train_00000000.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Quality issues are too severe to recognize vis...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>A bottle of spices in a plastic container layi...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>some basil leaves in a container on a counter</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>VizWiz_train_00000001.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>A kitchen counter the various items on top inc...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>a black tin of Coca Cola placed on a black sur...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Black counter with canisters, kettle and can o...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>VizWiz_train_00000002.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>a can of crushed tomatoes in puree from price ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>a Price Chopper branded can of crushed tomatoes</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Image is a can of crushed tomatoes in view.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>VizWiz_train_00000003.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>Screenshot from a smartphone with a case insen...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>image shows a screenshot of a page required ca...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A screenshot of Spotify page on a cell phone s...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>VizWiz_train_00000004.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>A garden book is sitting on a person's lap.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>a box for a solar garden light laying on someo...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A blue and yellow box with lights for the gard...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                  file_name  \\\n",
       "0         0  VizWiz_train_00000000.jpg   \n",
       "1         1  VizWiz_train_00000001.jpg   \n",
       "2         2  VizWiz_train_00000002.jpg   \n",
       "3         3  VizWiz_train_00000003.jpg   \n",
       "4         4  VizWiz_train_00000004.jpg   \n",
       "\n",
       "                                          vizwiz_url  text_detected  \\\n",
       "0  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "1  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "2  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "3  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "4  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "\n",
       "   unrecognizable  framing  blur  obstruction  rotation  too dark  ...  \\\n",
       "0               1        3     1            0         0         0  ...   \n",
       "1               0        0     5            0         0         0  ...   \n",
       "2               0        0     0            0         0         0  ...   \n",
       "3               0        0     0            0         0         0  ...   \n",
       "4               0        3     0            0         0         0  ...   \n",
       "\n",
       "   is_rejected_2                                    human_caption_3  \\\n",
       "0          False  Quality issues are too severe to recognize vis...   \n",
       "1          False  A kitchen counter the various items on top inc...   \n",
       "2          False  a can of crushed tomatoes in puree from price ...   \n",
       "3          False  Screenshot from a smartphone with a case insen...   \n",
       "4          False        A garden book is sitting on a person's lap.   \n",
       "\n",
       "   is_precanned_3 is_rejected_3  \\\n",
       "0            True          True   \n",
       "1           False         False   \n",
       "2           False         False   \n",
       "3           False         False   \n",
       "4           False         False   \n",
       "\n",
       "                                     human_caption_4  is_precanned_4  \\\n",
       "0  A bottle of spices in a plastic container layi...           False   \n",
       "1  a black tin of Coca Cola placed on a black sur...           False   \n",
       "2    a Price Chopper branded can of crushed tomatoes           False   \n",
       "3  image shows a screenshot of a page required ca...           False   \n",
       "4  a box for a solar garden light laying on someo...           False   \n",
       "\n",
       "  is_rejected_4                                    human_caption_5  \\\n",
       "0         False      some basil leaves in a container on a counter   \n",
       "1         False  Black counter with canisters, kettle and can o...   \n",
       "2         False        Image is a can of crushed tomatoes in view.   \n",
       "3         False  A screenshot of Spotify page on a cell phone s...   \n",
       "4         False  A blue and yellow box with lights for the gard...   \n",
       "\n",
       "   is_precanned_5 is_rejected_5  \n",
       "0           False         False  \n",
       "1           False         False  \n",
       "2           False         False  \n",
       "3           False         False  \n",
       "4           False         False  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_to_caption = generate_target_dataset(\n",
    "    \"../data/caption-dataset/annotations/train.json\",\n",
    "    \"../data/image-quality-assessment/annotations/train.json\",\n",
    ")\n",
    "dataset_to_caption_df = pd.DataFrame.from_dict(dataset_to_caption)\n",
    "dataset_to_caption_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset which images we want to\n",
    "There's no need to caption all the images with the set of prompts we want to try. Allow the user to select which images to try using `file_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_subset = [\n",
    "    \"VizWiz_train_00002685.jpg\",\n",
    "    \"VizWiz_train_00004360.jpg\",\n",
    "    \"VizWiz_train_00005236.jpg\",\n",
    "    \"VizWiz_train_00018277.jpg\",\n",
    "    \"VizWiz_train_00008241.jpg\",\n",
    "    \"VizWiz_train_00015593.jpg\",\n",
    "    \"VizWiz_train_00005191.jpg\",\n",
    "    \"VizWiz_train_00002033.jpg\",\n",
    "    \"VizWiz_train_00012474.jpg\",\n",
    "    \"VizWiz_train_00004571.jpg\",\n",
    "    \"VizWiz_train_00020191.jpg\",\n",
    "    \"VizWiz_train_00016717.jpg\",\n",
    "    \"VizWiz_train_00007812.jpg\",\n",
    "    \"VizWiz_train_00000428.jpg\",\n",
    "    \"VizWiz_train_00011077.jpg\",\n",
    "    \"VizWiz_train_00012338.jpg\",\n",
    "    \"VizWiz_train_00012570.jpg\",\n",
    "    \"VizWiz_train_00009241.jpg\",\n",
    "    \"VizWiz_train_00002353.jpg\",\n",
    "    \"VizWiz_train_00000575.jpg\",\n",
    "    \"VizWiz_train_00013633.jpg\",\n",
    "    \"VizWiz_train_00008078.jpg\",\n",
    "    \"VizWiz_train_00000140.jpg\",\n",
    "    \"VizWiz_train_00008867.jpg\",\n",
    "    \"VizWiz_train_00000078.jpg\",\n",
    "    \"VizWiz_train_00000192.jpg\",\n",
    "    \"VizWiz_train_00011842.jpg\",\n",
    "    \"VizWiz_train_00022167.jpg\",\n",
    "    \"VizWiz_train_00003433.jpg\",\n",
    "    \"VizWiz_train_00017296.jpg\",\n",
    "    \"VizWiz_train_00010915.jpg\",\n",
    "    \"VizWiz_train_00000145.jpg\",\n",
    "    \"VizWiz_train_00008162.jpg\",\n",
    "    \"VizWiz_train_00005952.jpg\",\n",
    "    \"VizWiz_train_00000139.jpg\",\n",
    "    \"VizWiz_train_00012215.jpg\",\n",
    "    \"VizWiz_train_00004920.jpg\",\n",
    "    \"VizWiz_train_00010826.jpg\",\n",
    "    \"VizWiz_train_00006570.jpg\",\n",
    "    \"VizWiz_train_00000070.jpg\",\n",
    "    \"VizWiz_train_00009603.jpg\",\n",
    "    \"VizWiz_train_00023166.jpg\",\n",
    "    \"VizWiz_train_00000621.jpg\",\n",
    "    \"VizWiz_train_00010902.jpg\",\n",
    "    \"VizWiz_train_00000131.jpg\",\n",
    "    \"VizWiz_train_00017219.jpg\",\n",
    "    \"VizWiz_train_00015043.jpg\",\n",
    "    \"VizWiz_train_00000051.jpg\",\n",
    "    \"VizWiz_train_00000149.jpg\",\n",
    "    \"VizWiz_train_00012203.jpg\",\n",
    "    \"VizWiz_train_00001485.jpg\",\n",
    "    \"VizWiz_train_00000142.jpg\",\n",
    "    \"VizWiz_train_00000126.jpg\",\n",
    "    \"VizWiz_train_00000155.jpg\",\n",
    "    \"VizWiz_train_00000558.jpg\",\n",
    "    \"VizWiz_train_00000560.jpg\",\n",
    "    \"VizWiz_train_00000637.jpg\",\n",
    "    \"VizWiz_train_00000506.jpg\",\n",
    "    \"VizWiz_train_00000193.jpg\",\n",
    "    \"VizWiz_train_00006773.jpg\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filtered_df = dataset_to_caption_df[\n",
    "    dataset_to_caption_df[\"file_name\"].isin(image_subset)\n",
    "]\n",
    "dataset_filtered = dataset_filtered_df.to_dict(orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processor\n",
    "Below, we process all selected data across all prompts. Some constants across runs:\n",
    "1. `temperature = 1.0` which should balance determinism and randomness.\n",
    "2. `use_cache = False` to make sure new responses are always generated.\n",
    "3. `do_sample = False` to use greedy decoding to reduce sampling variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # (\"plain\", \"What's in this image?\"),\n",
    "    # (\"plain-succinct\", \"What is in this image? Please provide a succinct description.\"),\n",
    "    # (\"blv-baseline\", \"Describe the image for blind and low-vision users.\"),\n",
    "    # (\n",
    "    #     \"original\",\n",
    "    #     \"\"\"You are a program designed to help blind and low-vision users understand images. Generate an accessible image description that includes key visual and contextual details of the image for blind and low-vision people.\n",
    "    #     Focus on the following principles:\n",
    "    #     • Clarity and Conciseness: Use simple, straightforward language to describe the main subjects and their relationships.;\n",
    "    #     • Relevance: Highlight only essential visual elements that contribute to understanding the image or its purpose.;\n",
    "    #     • Context: Provide contextual information when necessary, such as emotional tone, setting, or action. Avoid assumptions or subjective interpretations.;\n",
    "    #     • Specificity: Include important details like colors, shapes, textures, or text visible in the image, if relevant. Avoid overly general terms or unnecessary details.\n",
    "    #     Once you generate your caption, shorten it to a succinct, single sentence. Output only the final sentence.\n",
    "    #     \"\"\",\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"object-specific\",\n",
    "    #     \"\"\"You are a program designed to provide concise and accessible descriptions of objects in images for blind and low-vision people. Please describe the main objects in the image, their relationships to each other, and any essential details (e.g., colors, shapes, textures, or relevant text) necessary for understanding what the objects are.\n",
    "    #     Adhere to these guidelines:\n",
    "    #     • Clarity and Conciseness: Use simple, straightforward language, and omit unnecessary details.\n",
    "    #     • Relevance: Include only significant visual elements that convey the essential information about objects in the image.\n",
    "    #     • Context: Provide setting or emotional tone only when it’s clearly depicted, avoiding assumptions or subjective interpretations.\n",
    "    #     • Specificity: Mention key colors, shapes, visible text, or other notable features only if they enhance understanding.\n",
    "    #     After creating the full description, refine and compress it into a concise caption. Output only final caption.\n",
    "    #     \"\"\",\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"object-specific-structure\",\n",
    "    #     \"\"\"You are a program designed to describe objects in images to a blind and low-vision person. Please describe the main objects in the image, their relationships to each other, and any essential details (e.g., relevant text, shapes, colors, or textures) necessary for identifying what the objects are.\n",
    "    #     Adhere to these guidelines:\n",
    "    #     • Relevance: Include only significant visual elements that convey the essential information about objects in the image.\n",
    "    #     • Specificity: Mention visible text (including brand names), shapes, colors, textures, or notable features only if they enhance understanding.\n",
    "    #     • Structure of Response: First, mention what the object is. Then, provide details helpful for identifying the object, including text or shapes and colors. Finally, add details about the surrounding environment.\n",
    "    #     • Clarity and Conciseness: Use simple, straightforward language. Do not include unnecessary details.\n",
    "    #     After creating the full description, refine and compress it into a concise caption. Output only final caption.\n",
    "    #     \"\"\",\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"single-object-specific\",\n",
    "    #     \"\"\"You are a program designed to describe objects in images to a blind and low-vision person. Please describe the object in the image and any essential details (e.g., relevant text, shapes, colors, or textures) necessary for identifying the object.\n",
    "    #     Adhere to these guidelines:\n",
    "    #     • Relevance: Include only significant visual elements that convey the essential information about the main object in the image.\n",
    "    #     • Specificity: Mention visible text (including brand names), shapes, colors, textures, or notable features only if they enhance understanding.\n",
    "    #     • Structure of Response: First, mention what the object is. Then, provide details helpful for identifying the object, including text or shapes and colors. Finally, add details about the surrounding environment, if relevant.\n",
    "    #     • Clarity and Conciseness: Use simple, straightforward language. Do not include unnecessary details.\n",
    "    #     After creating the full description, refine and compress it into a concise 1-2 sentence caption. Output only the final caption.\n",
    "    #     \"\"\",\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"single-object-specific-v3\",\n",
    "    #     \"\\n\".join(\n",
    "    #         [\n",
    "    #             \"You are an assistant who describes objects in images to a blind and low-vision person. Please describe the object in the image and any essential details necessary for identifying the object.\",\n",
    "    #             \"\",\n",
    "    #             \"Follow these guidelines:\",\n",
    "    #             \"- Relevance and Specificity: Include visible text (including brand names), shapes, colors, textures, spatial relationships, or notable features only if they convey essential information about what is in the image.\",\n",
    "    #             \"- Structure of Response: Provide an overall description of the object, then include details to help identify it. Only include details about the surrounding environment if it helps to identify the object.\",\n",
    "    #             \"- Clarity: Use simple, straightforward, objective language. Avoid unnecessary details.\",\n",
    "    #             '- Format: Describe the object with a concise, 1-2 sentence caption. DO NOT mention camera blur or if an object is partially visible. DO NOT use \"it\" to refer to the object. DO NOT include statements like \"The image shows\" or \"The object is\".',\n",
    "    #             \"\",\n",
    "    #             \"Output only the final caption.\",\n",
    "    #         ]\n",
    "    #     ),\n",
    "    # ),\n",
    "    (\n",
    "        \"single-object-specific-v4\",\n",
    "        \"\\n\".join(\n",
    "            [\n",
    "                \"You are a helpful assistant who describes objects in images to a blind and low-vision person. Please describe the objects in the image and any essential details necessary for identifying them.\",\n",
    "                \"\",\n",
    "                \"Follow these guidelines:\",\n",
    "                \"- Relevance and Specificity: Include visible text (including brand names), shapes, colors, textures, spatial relationships, or notable features only if they convey essential information about what is in the image.\",\n",
    "                \"- Structure of Response: Provide a description of the object with essential details. Only include details about the surrounding environment if it helps to identify the object.\",\n",
    "                \"- Clarity: Use simple, straightforward, objective language. Avoid unnecessary details.\",\n",
    "                '- Format: Describe the object with a concise, 1-2 sentence caption. DO NOT mention camera blur or if an object is partially visible. DO NOT use \"it\" to refer to the object. DO NOT include statements like \"The image shows\" or \"The object is\".',\n",
    "                \"\",\n",
    "                \"Output only the final caption.\",\n",
    "            ]\n",
    "        ),\n",
    "    ),\n",
    "    # (\n",
    "    #     \"no-blv-ref\",\n",
    "    #     \"\"\"You are a program designed to help users understand images. Generate an image description that includes key visual and contextual details of the image. Focus on the following principles:\n",
    "    #     Clarity and Conciseness: Use simple, straightforward language to describe the main subjects and their relationships.;\n",
    "    #     Relevance: Highlight only essential visual elements that contribute to understanding the image or its purpose.;\n",
    "    #     Context: Provide contextual information when necessary, such as emotional tone, setting, or action. Avoid assumptions or subjective interpretations.;\n",
    "    #     Specificity: Include important details like colors, shapes, textures, or text visible in the image, if relevant. Avoid overly general terms or unnecessary details.\n",
    "    #     Once you generate your caption, shorten it to a succinct, single sentence. Output only the final sentence.\n",
    "    #     \"\"\",\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"abstain\",\n",
    "    #     \"You are a helpful assistant for describing images for blind and low-vision individuals. Do not hallucinate with incorrect answers if the image is indescribable. An image is indescribable if the provided image is too blurry, too bright or dark, obstructed, or too ill-framed to recognize correctly. Abstain from providing descriptions if the question is unanswerable.\",\n",
    "    # ),\n",
    "    # (\n",
    "    #     \"abstain-v2\",\n",
    "    #     \"\"\"You are a program designed to help users understand images. Generate an image description that includes key visual and contextual details of the image. Focus on the following principles:\n",
    "    #     Clarity and Conciseness: Use simple, straightforward language to describe the main subjects and their relationships.;\n",
    "    #     Relevance: Highlight only essential visual elements that contribute to understanding the image or its purpose.;\n",
    "    #     Context: Provide contextual information when necessary, such as emotional tone, setting, or action. Avoid assumptions or subjective interpretations.;\n",
    "    #     Specificity: Include important details like colors, shapes, textures, or text visible in the image, if relevant. Avoid overly general terms or unnecessary details.\n",
    "    #     Abstain: Do not hallucinate with incorrect answers if the image is too blurry, too bright or dark, obstructed, or too ill-framed to recognize correctly. Attempt to generate a caption before abstaining.\n",
    "    #     Once you generate your caption, shorten it to a succinct, single sentence. Output only the final sentence.\n",
    "    #     \"\"\",\n",
    "    # ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant who describes objects in images to a blind and low-vision person. Please describe the objects in the image and any essential details necessary for identifying them.\n",
      "\n",
      "Follow these guidelines:\n",
      "- Relevance and Specificity: Include visible text (including brand names), shapes, colors, textures, spatial relationships, or notable features only if they convey essential information about what is in the image.\n",
      "- Structure of Response: Provide a description of the object with essential details. Only include details about the surrounding environment if it helps to identify the object.\n",
      "- Clarity: Use simple, straightforward, objective language. Avoid unnecessary details.\n",
      "- Format: Describe the object with a concise, 1-2 sentence caption. DO NOT mention camera blur or if an object is partially visible. DO NOT use \"it\" to refer to the object. DO NOT include statements like \"The image shows\" or \"The object is\".\n",
      "\n",
      "Output only the final caption.\n"
     ]
    }
   ],
   "source": [
    "print(prompts[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e785774f334c42d7bfda975a44c3cd16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m captions_per_prompt = \u001b[43mgenerate_caption_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdataset_filtered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../data/caption-dataset/train\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m pd.DataFrame.from_dict(captions_per_prompt)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36mgenerate_caption_output\u001b[39m\u001b[34m(image_captioning_input, prompts, image_folder, scratch_path, use_scratch)\u001b[39m\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# repeat per prompt\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m prompt_name, prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[32m    207\u001b[39m     \u001b[38;5;66;03m# generate caption and store for output\u001b[39;00m\n\u001b[32m    208\u001b[39m     caption_output[index][\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mcaption_for_prompt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m] = (\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m         \u001b[43mgenerate_caption\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m     )\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# save scratch file for every 100 images\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_scratch:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mgenerate_caption\u001b[39m\u001b[34m(image_object, model, processor, prompt, temperature, do_sample)\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# generate output; maximum 300 new tokens; stop generation when <|endoftext|> is generated\u001b[39;00m\n\u001b[32m     28\u001b[39m output = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_from_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mGenerationConfig\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_strings\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m<|endoftext|>\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# only get generated tokens; decode them to text\u001b[39;00m\n\u001b[32m     39\u001b[39m generated_tokens = output[\u001b[32m0\u001b[39m, inputs[\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m].size(\u001b[32m1\u001b[39m) :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-O-0924/0e727957abd46f3ef741ddbda3452db1df873a6e/modeling_molmo.py:2212\u001b[39m, in \u001b[36mMolmoForCausalLM.generate_from_batch\u001b[39m\u001b[34m(self, batch, generation_config, **kwargs)\u001b[39m\n\u001b[32m   2209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2210\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m attention_mask.shape == (batch_size, mask_len)\n\u001b[32m-> \u001b[39m\u001b[32m2212\u001b[39m out = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2218\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_input_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_input_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2219\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2220\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2221\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2222\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/transformers/generation/utils.py:2255\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2247\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2248\u001b[39m         input_ids=input_ids,\n\u001b[32m   2249\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2250\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2251\u001b[39m         **model_kwargs,\n\u001b[32m   2252\u001b[39m     )\n\u001b[32m   2254\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2255\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2256\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2260\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2262\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2263\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2266\u001b[39m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[32m   2267\u001b[39m     beam_scorer = BeamSearchScorer(\n\u001b[32m   2268\u001b[39m         batch_size=batch_size,\n\u001b[32m   2269\u001b[39m         num_beams=generation_config.num_beams,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2274\u001b[39m         max_length=generation_config.max_length,\n\u001b[32m   2275\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/transformers/generation/utils.py:3257\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3255\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3257\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   3259\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3260\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3261\u001b[39m     outputs,\n\u001b[32m   3262\u001b[39m     model_kwargs,\n\u001b[32m   3263\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3264\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/accelerate/hooks.py:176\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    174\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-O-0924/0e727957abd46f3ef741ddbda3452db1df873a6e/modeling_molmo.py:2106\u001b[39m, in \u001b[36mMolmoForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, inputs_embeds, attention_mask, attention_bias, response_mask, images, image_masks, image_input_idx, subsegment_ids, position_ids, past_key_values, labels, loss_masks, use_cache, last_logits_only, output_attentions, output_hidden_states, append_last_valid_logits, return_dict, cache_position)\u001b[39m\n\u001b[32m   2103\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   2105\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2106\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2107\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2108\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2109\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2110\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2111\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2112\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2113\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2114\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_input_idx\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_input_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2115\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubsegment_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubsegment_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2116\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2117\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2118\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2119\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlast_logits_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlast_logits_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2120\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mappend_last_valid_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2122\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2124\u001b[39m logits = outputs.logits\n\u001b[32m   2125\u001b[39m hidden_states = outputs.hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-O-0924/0e727957abd46f3ef741ddbda3452db1df873a6e/modeling_molmo.py:1954\u001b[39m, in \u001b[36mMolmo.forward\u001b[39m\u001b[34m(self, input_ids, input_embeddings, attention_mask, attention_bias, response_mask, images, image_masks, image_input_idx, subsegment_ids, position_ids, past_key_values, use_cache, last_logits_only, output_hidden_states, append_last_valid_logits)\u001b[39m\n\u001b[32m   1951\u001b[39m     all_hidden_states.append(x)\n\u001b[32m   1953\u001b[39m layer_past = \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m past_key_values[block_idx]\n\u001b[32m-> \u001b[39m\u001b[32m1954\u001b[39m x, cache = \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1956\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attn_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1957\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/allenai/Molmo-7B-O-0924/0e727957abd46f3ef741ddbda3452db1df873a6e/modeling_molmo.py:520\u001b[39m, in \u001b[36mMolmoSequentialBlock.forward\u001b[39m\u001b[34m(self, x, attention_bias, position_ids, layer_past, use_cache)\u001b[39m\n\u001b[32m    517\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    518\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.ff_norm(x)\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._activation_checkpoint_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    522\u001b[39m     x = \u001b[38;5;28mself\u001b[39m._activation_checkpoint_fn(\u001b[38;5;28mself\u001b[39m.act, x)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/accelerate/hooks.py:171\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(module, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     args, kwargs = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_hf_hook\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpre_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m module._hf_hook.no_grad:\n\u001b[32m    173\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/accelerate/hooks.py:342\u001b[39m, in \u001b[36mAlignDevicesHook.pre_forward\u001b[39m\u001b[34m(self, module, *args, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, _ \u001b[38;5;129;01min\u001b[39;00m named_module_tensors(\n\u001b[32m    336\u001b[39m     module,\n\u001b[32m    337\u001b[39m     include_buffers=\u001b[38;5;28mself\u001b[39m.offload_buffers,\n\u001b[32m    338\u001b[39m     recurse=\u001b[38;5;28mself\u001b[39m.place_submodules,\n\u001b[32m    339\u001b[39m     remove_non_persistent=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    340\u001b[39m ):\n\u001b[32m    341\u001b[39m     fp16_statistics = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m     value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweights_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    343\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m name \u001b[38;5;129;01mand\u001b[39;00m name.replace(\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSCB\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.weights_map.keys():\n\u001b[32m    344\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m value.dtype == torch.int8:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/accelerate/utils/offload.py:118\u001b[39m, in \u001b[36mPrefixedDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mkey\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/virtualenvs/local-blurred-captioning-exploration-4LZKhcfn/lib/python3.11/site-packages/accelerate/utils/offload.py:171\u001b[39m, in \u001b[36mOffloadedWeightsLoader.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=device) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m         tensor = \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight_info\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    173\u001b[39m     \u001b[38;5;66;03m# if failed to get_tensor on the device, such as bf16 on mps, try to load it on CPU first\u001b[39;00m\n\u001b[32m    174\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(weight_info[\u001b[33m\"\u001b[39m\u001b[33msafetensors_file\u001b[39m\u001b[33m\"\u001b[39m], framework=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "captions_per_prompt = generate_caption_output(\n",
    "    dataset_filtered, prompts, \"../data/caption-dataset/train\"\n",
    ")\n",
    "\n",
    "pd.DataFrame.from_dict(captions_per_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>file_name</th>\n",
       "      <th>vizwiz_url</th>\n",
       "      <th>text_detected</th>\n",
       "      <th>unrecognizable</th>\n",
       "      <th>framing</th>\n",
       "      <th>blur</th>\n",
       "      <th>obstruction</th>\n",
       "      <th>rotation</th>\n",
       "      <th>too dark</th>\n",
       "      <th>...</th>\n",
       "      <th>human_caption_3</th>\n",
       "      <th>is_precanned_3</th>\n",
       "      <th>is_rejected_3</th>\n",
       "      <th>human_caption_4</th>\n",
       "      <th>is_precanned_4</th>\n",
       "      <th>is_rejected_4</th>\n",
       "      <th>human_caption_5</th>\n",
       "      <th>is_precanned_5</th>\n",
       "      <th>is_rejected_5</th>\n",
       "      <th>caption_for_prompt_single-object-specific-v4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>51</td>\n",
       "      <td>VizWiz_train_00000051.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>tube package of a substance that is orange blu...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A person holding up a food item with a blue an...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>some sort of cream that someone is holding</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A person's hand is holding a cylindrical plast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70</td>\n",
       "      <td>VizWiz_train_00000070.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>A screen that is showing the frequency and aud...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A bright display saying something about FREDQU...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Quality issues are too severe to recognize vis...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>A blue sign with white text reading \"Frequency...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>VizWiz_train_00000078.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>A regular sized can of Mountain Dew missing it...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A can of Mountain Dew with the tab opener remo...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Image contain a partial part of a mountain dew...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A black aluminum soda can with a silver top an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>VizWiz_train_00000126.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>False</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Finger covers camera lens so that the photo is...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Quality issues are too severe to recognize vis...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>A orange on a gray cloth with the photographer...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A white cylindrical container with a black lid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>131</td>\n",
       "      <td>VizWiz_train_00000131.jpg</td>\n",
       "      <td>https://vizwiz.cs.colorado.edu/VizWiz_visualiz...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>Quality issues are too severe to recognize vis...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>A canister shaped object is near a wood wall.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>some type of liquid that is in a container</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>A white plastic bag with red text, likely a pr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                  file_name  \\\n",
       "0        51  VizWiz_train_00000051.jpg   \n",
       "1        70  VizWiz_train_00000070.jpg   \n",
       "2        78  VizWiz_train_00000078.jpg   \n",
       "3       126  VizWiz_train_00000126.jpg   \n",
       "4       131  VizWiz_train_00000131.jpg   \n",
       "\n",
       "                                          vizwiz_url  text_detected  \\\n",
       "0  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "1  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "2  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "3  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...          False   \n",
       "4  https://vizwiz.cs.colorado.edu/VizWiz_visualiz...           True   \n",
       "\n",
       "   unrecognizable  framing  blur  obstruction  rotation  too dark  ...  \\\n",
       "0               2        2     3            0         0         0  ...   \n",
       "1               2        3     5            1         4         1  ...   \n",
       "2               0        4     5            0         0         0  ...   \n",
       "3               3        1     4            5         0         0  ...   \n",
       "4               1        2     5            0         0         0  ...   \n",
       "\n",
       "                                     human_caption_3  is_precanned_3  \\\n",
       "0  tube package of a substance that is orange blu...           False   \n",
       "1  A screen that is showing the frequency and aud...           False   \n",
       "2  A regular sized can of Mountain Dew missing it...           False   \n",
       "3  Finger covers camera lens so that the photo is...           False   \n",
       "4  Quality issues are too severe to recognize vis...            True   \n",
       "\n",
       "   is_rejected_3                                    human_caption_4  \\\n",
       "0          False  A person holding up a food item with a blue an...   \n",
       "1          False  A bright display saying something about FREDQU...   \n",
       "2          False  A can of Mountain Dew with the tab opener remo...   \n",
       "3          False  Quality issues are too severe to recognize vis...   \n",
       "4          False      A canister shaped object is near a wood wall.   \n",
       "\n",
       "   is_precanned_4  is_rejected_4  \\\n",
       "0           False          False   \n",
       "1           False          False   \n",
       "2           False          False   \n",
       "3            True          False   \n",
       "4           False          False   \n",
       "\n",
       "                                     human_caption_5  is_precanned_5  \\\n",
       "0         some sort of cream that someone is holding           False   \n",
       "1  Quality issues are too severe to recognize vis...            True   \n",
       "2  Image contain a partial part of a mountain dew...           False   \n",
       "3  A orange on a gray cloth with the photographer...           False   \n",
       "4         some type of liquid that is in a container           False   \n",
       "\n",
       "   is_rejected_5       caption_for_prompt_single-object-specific-v4  \n",
       "0          False  A person's hand is holding a cylindrical plast...  \n",
       "1          False  A blue sign with white text reading \"Frequency...  \n",
       "2          False  A black aluminum soda can with a silver top an...  \n",
       "3          False  A white cylindrical container with a black lid...  \n",
       "4          False  A white plastic bag with red text, likely a pr...  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "output_df = pd.DataFrame.from_dict(captions_per_prompt)\n",
    "\n",
    "# save file\n",
    "outdir = \"../data/labeled-data/molmo-model/prompt-testing\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "output_df.to_csv(\n",
    "    f\"{outdir}/prompt-testing-round7_32bit_03-25-25.csv\",\n",
    "    index=False,\n",
    ")\n",
    "\n",
    "# print dataframe\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if processor is not None:\n",
    "    del processor\n",
    "if model is not None:\n",
    "    del model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-blurred-captioning-exploration-4LZKhcfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
