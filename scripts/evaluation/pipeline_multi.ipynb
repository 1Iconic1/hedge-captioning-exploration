{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library for cap_f1\n",
    "from cap_f1 import *\n",
    "from datetime import datetime\n",
    "from multiprocessing import Pool\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "# code for no need for restarting the kernel when python file is updated\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load caption file...\n"
     ]
    }
   ],
   "source": [
    "print(\"Load caption file...\")\n",
    "\n",
    "# features that we need to extract from the original dataset\n",
    "org_caption_dataset = read_json(\"evaluation_results_5429-images_2025-04-03_11_27_fixed.json\")\n",
    "\n",
    "all_human_captions=[]\n",
    "for item in org_caption_dataset:\n",
    "    # Filter out human captions\n",
    "    human_captions = [\n",
    "        hc[\"caption\"]            \n",
    "        for hc in item[\"human_captions\"]\n",
    "        if hc[\"caption\"] != \"Quality issues are too severe to recognize visual content.\"\n",
    "    ]\n",
    "    all_human_captions.append(human_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "# This is for testing\n",
    "print(len(org_caption_dataset))\n",
    "org_caption_dataset = org_caption_dataset[:32]\n",
    "all_human_captions = all_human_captions[:32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Multi Processors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(start_idx, end_idx, org_caption_dataset, all_human_captions, folder_path, timestamp, chunk_id):\n",
    "    subset = org_caption_dataset[start_idx:end_idx]\n",
    "    LIMIT = len(subset)\n",
    "    human_subset = all_human_captions[start_idx:end_idx]\n",
    "\n",
    "    # Step 1: Parse atomics\n",
    "    T_atomics, g_atomics = generate_atomic_statement(subset, limit=LIMIT)\n",
    "    save_results_json(output_path=f\"{folder_path}/parsed_caption_{timestamp}_chunk{chunk_id}.json\",\n",
    "                      org_dataset=subset, T_atomics=T_atomics, g_atomics=g_atomics, limit=LIMIT)\n",
    "\n",
    "    # Step 2: Match human & generated\n",
    "    metadata = evaluate_matching(human_subset, T_atomics, g_atomics)\n",
    "    save_results_json(output_path=f\"{folder_path}/recall_precision_{timestamp}_chunk{chunk_id}.json\",\n",
    "                      update_existing=f\"{folder_path}/parsed_caption_{timestamp}_chunk{chunk_id}.json\",\n",
    "                      metadata=metadata, limit=LIMIT)\n",
    "\n",
    "    # Step 3: Cap F1\n",
    "    evaluation = calculate_cap_f1(metadata)\n",
    "    save_results_json(output_path=f\"{folder_path}/final_{timestamp}_chunk{chunk_id}.json\",\n",
    "                      update_existing=f\"{folder_path}/recall_precision_{timestamp}_chunk{chunk_id}.json\",\n",
    "                      evaluations=evaluation, limit=LIMIT)\n",
    "\n",
    "def run_parallel_processing(org_caption_dataset, all_human_captions, folder_path, timestamp, num_workers=32):\n",
    "    total = len(org_caption_dataset)\n",
    "    chunk_size = math.ceil(total / num_workers)\n",
    "\n",
    "    with Pool(processes=num_workers) as pool:\n",
    "        jobs = []\n",
    "        for i in range(num_workers):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, total)\n",
    "            jobs.append(pool.apply_async(process_batch, (start_idx, end_idx, org_caption_dataset, all_human_captions, folder_path, timestamp, i)))\n",
    "        \n",
    "        for job in jobs:\n",
    "            job.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk8.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:35<00:00,  8.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:39<00:00,  9.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:40<00:00, 10.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:41<00:00, 10.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:42<00:00, 10.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:45<00:00, 11.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/parsed_caption_2025-04-07_22-25_chunk3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:08<00:25,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Recall mismatch for model [gpt-4o-2024-08-06]\n",
      "length 13 vs 13\n",
      "T atomics:\n",
      "['There is a bottle of vitamins.', 'The bottle of vitamins is on white sheets.', 'There is an arm next to the bottle of vitamins.', 'The bottle has a green cap.', 'The bottle has leaves printed on the label.', 'The image describes the medicinal uses of the product.', 'The bottle is a supplement bottle.', 'The supplement is natural.', 'The supplement is goldenseal.', 'The bottle is turned on its side.', 'The object is soft.', 'The object is pillow-like.', \"The letters on the bottle are 'denseal'.\"]\n",
      "Recall TPs:\n",
      "['There is a bottle of vitamins.', 'The bottle has a green cap.', 'The bottle is on white sheets.']\n",
      "Recall FNs:\n",
      "['There is an arm next to the bottle of vitamins.', 'The bottle has leaves printed on the label.', 'The image describes the medicinal uses of the product.', 'The bottle is a supplement bottle.', 'The supplement is natural.', 'The supplement is goldenseal.', 'The bottle is turned on its side.', 'The object is soft.', 'The object is pillow-like.', \"The letters on the bottle are 'denseal'.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:36<00:00,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 86037.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk7.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:37<00:00,  9.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 39475.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:40<00:00, 10.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 86480.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk6.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:39<00:00,  9.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 40329.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk4.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:46<00:00, 11.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 40920.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk5.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:46<00:00, 11.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 67108.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk1.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:43<00:00, 10.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 42153.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk3.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:49<00:00, 12.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/recall_precision_2025-04-07_22-25_chunk0.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 88768.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON to: results/2025-04-07_22-25/final_2025-04-07_22-25_chunk0.json\n"
     ]
    }
   ],
   "source": [
    "# for filename\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d_%H-%M\")\n",
    "\n",
    "#create folder to save the results\n",
    "folder_path = f\"results/{timestamp}\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "run_parallel_processing(org_caption_dataset, all_human_captions, folder_path, timestamp, num_workers=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged 32 entries into results/2025-04-07_22-25/__final_2025-04-07_22-25_merged.json\n"
     ]
    }
   ],
   "source": [
    "def merge_json_chunks(output_file, file_pattern):\n",
    "    merged_data = []\n",
    "\n",
    "    for filename in sorted(glob.glob(file_pattern)):\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    merged_data.extend(data)\n",
    "                elif isinstance(data, dict):\n",
    "                    merged_data.append(data)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read {filename}: {e}\")\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out_f:\n",
    "        json.dump(merged_data, out_f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Merged {len(merged_data)} entries into {output_file}\")\n",
    "\n",
    "merge_json_chunks(\n",
    "    output_file=f\"{folder_path}/__final_{timestamp}_merged.json\",\n",
    "    file_pattern=f\"{folder_path}/final_{timestamp}_chunk*.json\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file saved to: results/2025-04-05_16-36/__final_2025-04-05_16-36_merged.csv\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "\n",
    "json_path = f\"{folder_path}/__final_{timestamp}_merged.json\"\n",
    "csv_path = f\"{folder_path}/__final_{timestamp}_merged.csv\"\n",
    "\n",
    "with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "fieldnames = [\n",
    "    \"image\",\n",
    "    \"link\",\n",
    "    \"T_atomics\",\n",
    "    \"gpt_caption\",\n",
    "    \"gpt_g_atomics\",\n",
    "    \"gpt_recall_TPs\",\n",
    "    \"gpt_recall_FNs\",\n",
    "    \"gpt_precision_TPs\",\n",
    "    \"gpt_precision_FPs\",\n",
    "    \"molmo_caption\",\n",
    "    \"molmo_g_atomics\",\n",
    "    \"molmo_recall_TPs\",\n",
    "    \"molmo_recall_FNs\",\n",
    "    \"molmo_precision_TPs\",\n",
    "    \"molmo_precision_FPs\",\n",
    "    \"llama_caption\",\n",
    "    \"llama_g_atomics\",\n",
    "    \"llama_recall_TPs\",\n",
    "    \"llama_recall_FNs\",\n",
    "    \"llama_precision_TPs\",\n",
    "    \"llama_precision_FPs\",\n",
    "    \"gpt_recall\",\n",
    "    \"gpt_precision\",\n",
    "    \"gpt_capf1\",\n",
    "    \"molmo_recall\",\n",
    "    \"molmo_precision\",\n",
    "    \"molmo_capf1\",\n",
    "    \"llama_recall\",\n",
    "    \"llama_precision\",\n",
    "    \"llama_capf1\",\n",
    "]\n",
    "\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for item in data:\n",
    "        file_name = item.get(\"file_name\", \"\")\n",
    "        cap_f1 = item.get(\"evaluation\", {}).get(\"cap_f1\", {})\n",
    "        scores = cap_f1.get(\"scores\", {})\n",
    "        metadata = cap_f1.get(\"metadata\", {})\n",
    "        t_atomics = cap_f1.get(\"T_atomics\", [])\n",
    "\n",
    "        model_keys = {\n",
    "            \"gpt\": \"gpt-4o-2024-08-06\",\n",
    "            \"molmo\": \"Molmo-7B-O-0924\",\n",
    "            \"llama\": \"Llama-3.2-11B-Vision-Instruct\",\n",
    "        }\n",
    "\n",
    "        row = {\n",
    "            \"image\": file_name,\n",
    "            \"link\": f'=HYPERLINK(\"https://vizwiz.cs.colorado.edu/VizWiz_visualization_img/{file_name}\", \"{file_name}\")',\n",
    "            \"T_atomics\": \"\\n\".join(t_atomics),\n",
    "            \"gpt_caption\": item[\"model_captions\"][0][\"caption\"],\n",
    "            \"gpt_g_atomics\": \"\",\n",
    "            \"gpt_recall_TPs\": \"\",\n",
    "            \"gpt_precision_TPs\": \"\",\n",
    "            \"molmo_caption\": item[\"model_captions\"][2][\"caption\"],\n",
    "            \"molmo_g_atomics\": \"\",\n",
    "            \"molmo_recall_TPs\": \"\",\n",
    "            \"molmo_precision_TPs\": \"\",\n",
    "            \"llama_caption\": item[\"model_captions\"][1][\"caption\"],\n",
    "            \"llama_g_atomics\": \"\",\n",
    "            \"llama_recall_TPs\": \"\",\n",
    "            \"llama_precision_TPs\": \"\",\n",
    "            \"gpt_recall\": scores.get(model_keys[\"gpt\"], {}).get(\"recall\"),\n",
    "            \"gpt_precision\": scores.get(model_keys[\"gpt\"], {}).get(\"precision\"),\n",
    "            \"gpt_capf1\": scores.get(model_keys[\"gpt\"], {}).get(\"cap_f1\"),\n",
    "            \"molmo_recall\": scores.get(model_keys[\"molmo\"], {}).get(\"recall\"),\n",
    "            \"molmo_precision\": scores.get(model_keys[\"molmo\"], {}).get(\"precision\"),\n",
    "            \"molmo_capf1\": scores.get(model_keys[\"molmo\"], {}).get(\"cap_f1\"),\n",
    "            \"llama_recall\": scores.get(model_keys[\"llama\"], {}).get(\"recall\"),\n",
    "            \"llama_precision\": scores.get(model_keys[\"llama\"], {}).get(\"precision\"),\n",
    "            \"llama_capf1\": scores.get(model_keys[\"llama\"], {}).get(\"cap_f1\"),\n",
    "        }\n",
    "\n",
    "        for short_name, model_key in model_keys.items():\n",
    "            # g_atomics\n",
    "            g_atomics_list = cap_f1.get(\"g_atomics\", {}).get(model_key, [])\n",
    "            row[f\"{short_name}_g_atomics\"] = \"\\n\".join(g_atomics_list)\n",
    "\n",
    "            # recall TPs\n",
    "            recall_tps = metadata.get(model_key, {}).get(\"recall\", {}).get(\"TPs\", [])\n",
    "            row[f\"{short_name}_recall_TPs\"] = \"\\n\".join(recall_tps)\n",
    "\n",
    "            # recall FNs\n",
    "            recall_fns = metadata.get(model_key, {}).get(\"recall\", {}).get(\"FNs\", [])\n",
    "            row[f\"{short_name}_recall_FNs\"] = \"\\n\".join(recall_fns)\n",
    "\n",
    "            # precision TPs\n",
    "            precision_tps = (\n",
    "                metadata.get(model_key, {}).get(\"precision\", {}).get(\"TPs\", [])\n",
    "            )\n",
    "            row[f\"{short_name}_precision_TPs\"] = \"\\n\".join(precision_tps)\n",
    "\n",
    "            # precision FPs\n",
    "            precision_fps = (\n",
    "                metadata.get(model_key, {}).get(\"precision\", {}).get(\"FPs\", [])\n",
    "            )\n",
    "            row[f\"{short_name}_precision_FPs\"] = \"\\n\".join(precision_fps)\n",
    "\n",
    "        writer.writerow(row)\n",
    "\n",
    "print(f\"CSV file saved to: {csv_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
